diff -r '--unified=1' -x '*.pyc' deepfloyd_if-orig/__init__.py deepfloyd_if/__init__.py
--- deepfloyd_if-orig/__init__.py	2023-06-05 09:05:38.091033700 +0400
+++ deepfloyd_if/__init__.py	2023-05-28 12:04:07.142713100 +0400
@@ -3,2 +3,2 @@

-__version__ = '1.0.1'
+__version__ = '1.0.2rc0'
diff -r '--unified=1' -x '*.pyc' deepfloyd_if-orig/model/gaussian_diffusion.py deepfloyd_if/model/gaussian_diffusion.py
--- deepfloyd_if-orig/model/gaussian_diffusion.py	2023-06-05 09:05:38.091033700 +0400
+++ deepfloyd_if/model/gaussian_diffusion.py	2023-05-29 18:46:43.121027700 +0400
@@ -487,2 +487,3 @@

+            n_indices = len(indices)
             indices = tqdm(indices)
@@ -503,2 +504,6 @@
                 )
+
+                if callable(progress):
+                    progress(n_indices, i)
+
                 yield out
@@ -666,2 +671,3 @@

+            n_indices = len(indices)
             indices = tqdm(indices)
@@ -682,2 +688,6 @@
                 )
+
+                if callable(progress):
+                    progress(n_indices, i)
+
                 yield out
diff -r '--unified=1' -x '*.pyc' deepfloyd_if-orig/modules/base.py deepfloyd_if/modules/base.py
--- deepfloyd_if-orig/modules/base.py	2023-06-05 09:05:38.091033700 +0400
+++ deepfloyd_if/modules/base.py	2023-06-04 16:18:41.913616300 +0400
@@ -179,4 +179,8 @@
         else:
+            if support_noise.shape != (1, 3, image_h, image_w):
+                print("Please try with a square image.")
+                print(f"Support noise shape mismatch: {support_noise.shape} != {(1, 3, image_h, image_w)}")
             assert support_noise_less_qsample_steps < len(diffusion.timestep_map) - 1
             assert support_noise.shape == (1, 3, image_h, image_w)
+
             q_sample_steps = torch.tensor([int(len(diffusion.timestep_map) - 1 - support_noise_less_qsample_steps)])
diff -r '--unified=1' -x '*.pyc' deepfloyd_if-orig/modules/t5.py deepfloyd_if/modules/t5.py
--- deepfloyd_if-orig/modules/t5.py	2023-06-05 09:05:38.091033700 +0400
+++ deepfloyd_if/modules/t5.py	2023-05-28 12:04:07.147074000 +0400
@@ -64,3 +64,3 @@

-        path = dir_or_name
+        tokenizer_path, path = dir_or_name, dir_or_name
         if dir_or_name in self.available_models:
@@ -73,5 +73,13 @@
                                 force_filename=filename, token=self.hf_token)
-            path = cache_dir
+            tokenizer_path, path = cache_dir, cache_dir
+        else:
+            cache_dir = os.path.join(self.cache_dir, 't5-v1_1-xxl')
+            for filename in [
+                'config.json', 'special_tokens_map.json', 'spiece.model', 'tokenizer_config.json',
+            ]:
+                hf_hub_download(repo_id='DeepFloyd/t5-v1_1-xxl', filename=filename, cache_dir=cache_dir,
+                                force_filename=filename, token=self.hf_token)
+            tokenizer_path = cache_dir

-        self.tokenizer = AutoTokenizer.from_pretrained(path)
+        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
         self.model = T5EncoderModel.from_pretrained(path, **t5_model_kwargs).eval()
diff -r '--unified=1' -x '*.pyc' deepfloyd_if-orig/pipelines/dream.py deepfloyd_if/pipelines/dream.py
--- deepfloyd_if-orig/pipelines/dream.py	2023-06-05 09:05:38.091033700 +0400
+++ deepfloyd_if/pipelines/dream.py	2023-06-04 16:12:41.378921200 +0400
@@ -62,3 +62,5 @@

-    t5_embs = t5.get_text_embeddings(prompt)
+    t5_embs = if_I_kwargs.get('t5_embs', None)
+    if t5_embs is None:
+        t5_embs = t5.get_text_embeddings(prompt)

@@ -73,3 +75,5 @@
             style_prompt = [style_prompt]
-        style_t5_embs = t5.get_text_embeddings(style_prompt)
+        style_t5_embs = if_I_kwargs.get('style_t5_embs', None)
+        if style_t5_embs is None:
+            style_t5_embs = t5.get_text_embeddings(style_prompt)
         if_I_kwargs['style_t5_embs'] = style_t5_embs
@@ -80,7 +84,17 @@
             negative_prompt = [negative_prompt]
-        negative_t5_embs = t5.get_text_embeddings(negative_prompt)
+        negative_t5_embs = if_I_kwargs.get('negative_t5_embs', None)
+        if negative_t5_embs is None:
+            negative_t5_embs = t5.get_text_embeddings(negative_prompt)
         if_I_kwargs['negative_t5_embs'] = negative_t5_embs

-    stageI_generations, _ = if_I.embeddings_to_image(**if_I_kwargs)
-    pil_images_I = if_I.to_images(stageI_generations, disable_watermark=disable_watermark)
+    pass_prompt_to_sIII = True
+    if if_I_kwargs and hasattr(if_I_kwargs, "imagesI"):
+        stageI_output = None
+        stageI_generations = if_I_kwargs.tensorsI
+        pil_images_I = if_I_kwargs.imagesI
+        pass_prompt_to_sIII = if_I_kwargs.pass_prompt_to_sIII
+    else:
+        stageI_output = if_I.embeddings_to_image(**if_I_kwargs)
+        stageI_generations, _ = stageI_output
+        pil_images_I = if_I.to_images(stageI_generations, disable_watermark=disable_watermark)

@@ -97,3 +111,4 @@

-        stageII_generations, _meta = if_II.embeddings_to_image(**if_II_kwargs)
+        stageII_output = if_II.embeddings_to_image(**if_II_kwargs)
+        stageII_generations, _meta = stageII_output
         pil_images_II = if_II.to_images(stageII_generations, disable_watermark=disable_watermark)
@@ -102,4 +117,6 @@
     else:
+        stageII_output = None
         stageII_generations = None

+    stageIII_output = None
     if if_II is not None and if_III is not None:
@@ -110,3 +127,3 @@
             if if_III.use_diffusers:
-                if_III_kwargs['prompt'] = prompt[idx: idx+1]
+                if_III_kwargs['prompt'] = prompt[idx: idx+1] if pass_prompt_to_sIII else ['']

@@ -125,3 +142,4 @@

-            _stageIII_generations, _meta = if_III.embeddings_to_image(**if_III_kwargs)
+            stageIII_output = if_III.embeddings_to_image(**if_III_kwargs)
+            _stageIII_generations, _meta = stageIII_output
             stageIII_generations.append(_stageIII_generations)
@@ -136,2 +154,3 @@
     if return_tensors:
+        result["output"] = (stageI_output, stageII_output, stageIII_output)
         return result, (stageI_generations, stageII_generations, stageIII_generations)
diff -r '--unified=1' -x '*.pyc' deepfloyd_if-orig/pipelines/inpainting.py deepfloyd_if/pipelines/inpainting.py
--- deepfloyd_if-orig/pipelines/inpainting.py	2023-06-05 09:05:38.091033700 +0400
+++ deepfloyd_if/pipelines/inpainting.py	2023-06-03 09:57:29.515723900 +0400
@@ -34,3 +34,5 @@

-    t5_embs = t5.get_text_embeddings(prompt)
+    t5_embs = if_I_kwargs.get('t5_embs', None)
+    if t5_embs is None:
+        t5_embs = t5.get_text_embeddings(prompt)

@@ -39,3 +41,5 @@
             negative_prompt = [negative_prompt]
-        negative_t5_embs = t5.get_text_embeddings(negative_prompt)
+        negative_t5_embs = if_I_kwargs.get('negative_t5_embs', None)
+        if negative_t5_embs is None:
+            negative_t5_embs = t5.get_text_embeddings(negative_prompt)
     else:
@@ -43,5 +47,5 @@

-    low_res = _prepare_pil_image(support_pil_img, 64)
-    mid_res = _prepare_pil_image(support_pil_img, 256)
-    high_res = _prepare_pil_image(support_pil_img, 1024)
+    low_res = getattr(if_I_kwargs, 'low_res') if hasattr(if_I_kwargs, 'low_res') else _prepare_pil_image(support_pil_img, 64)
+    mid_res = getattr(if_I_kwargs, 'mid_res') if hasattr(if_I_kwargs, 'mid_res') else _prepare_pil_image(support_pil_img, 256)
+    high_res = getattr(if_I_kwargs, 'high_res') if hasattr(if_I_kwargs, 'high_res') else _prepare_pil_image(support_pil_img, 1024)

@@ -60,9 +64,16 @@

-    inpainting_mask_I = img_as_bool(resize(inpainting_mask[0].cpu(), (3, image_h, image_w)))
-    inpainting_mask_I = torch.from_numpy(inpainting_mask_I).unsqueeze(0).to(if_I.device)
-
-    if_I_kwargs['inpainting_mask'] = inpainting_mask_I
-
-    stageI_generations, _ = if_I.embeddings_to_image(**if_I_kwargs)
-    pil_images_I = if_I.to_images(stageI_generations, disable_watermark=disable_watermark)
+    pass_prompt_to_sIII = True
+    if if_I_kwargs and hasattr(if_I_kwargs, "imagesI"):
+        stageI_generations = if_I_kwargs.tensorsI
+        pil_images_I = if_I_kwargs.imagesI
+        stageI_output = None
+        pass_prompt_to_sIII = if_I_kwargs.pass_prompt_to_sIII
+    else:
+        inpainting_mask_I = img_as_bool(resize(inpainting_mask, (3, image_h, image_w)))
+        inpainting_mask_I = torch.from_numpy(inpainting_mask_I).unsqueeze(0).to(if_I.device)
+        if_I_kwargs['inpainting_mask'] = inpainting_mask_I
+
+        stageI_output = if_I.embeddings_to_image(**if_I_kwargs)
+        stageI_generations, _ = stageI_output
+        pil_images_I = if_I.to_images(stageI_generations, disable_watermark=disable_watermark)

@@ -83,3 +94,3 @@
         if 'inpainting_mask' not in if_II_kwargs:
-            inpainting_mask_II = img_as_bool(resize(inpainting_mask[0].cpu(), (3, image_h, image_w)))
+            inpainting_mask_II = img_as_bool(resize(inpainting_mask, (3, image_h, image_w)))
             inpainting_mask_II = torch.from_numpy(inpainting_mask_II).unsqueeze(0).to(if_II.device)
@@ -87,3 +98,4 @@

-        stageII_generations, _meta = if_II.embeddings_to_image(**if_II_kwargs)
+        stageII_output = if_II.embeddings_to_image(**if_II_kwargs)
+        stageII_generations, _meta = stageII_output
         pil_images_II = if_II.to_images(stageII_generations, disable_watermark=disable_watermark)
@@ -92,2 +104,3 @@
     else:
+        stageII_output = None
         stageII_generations = None
@@ -101,3 +114,3 @@
             if if_III.use_diffusers:
-                if_III_kwargs['prompt'] = prompt[idx: idx+1]
+                if_III_kwargs['prompt'] = prompt[idx: idx+1] if pass_prompt_to_sIII else ['']

@@ -112,3 +125,3 @@
             if 'inpainting_mask' not in if_III_kwargs:
-                inpainting_mask_III = img_as_bool(resize(inpainting_mask[0].cpu(), (3, image_h, image_w)))
+                inpainting_mask_III = img_as_bool(resize(inpainting_mask, (3, image_h, image_w)))
                 inpainting_mask_III = torch.from_numpy(inpainting_mask_III).unsqueeze(0).to(if_III.device)
@@ -116,3 +129,4 @@

-            _stageIII_generations, _meta = if_III.embeddings_to_image(**if_III_kwargs)
+            stageIII_output = if_III.embeddings_to_image(**if_III_kwargs)
+            _stageIII_generations, _meta = stageIII_output
             stageIII_generations.append(_stageIII_generations)
@@ -124,2 +138,3 @@
     else:
+        stageIII_output = None
         stageIII_generations = None
@@ -127,2 +142,3 @@
     if return_tensors:
+        result["output"] = (stageI_output, stageII_output, stageIII_output)
         return result, (stageI_generations, stageII_generations, stageIII_generations)
diff -r '--unified=1' -x '*.pyc' deepfloyd_if-orig/pipelines/style_transfer.py deepfloyd_if/pipelines/style_transfer.py
--- deepfloyd_if-orig/pipelines/style_transfer.py	2023-06-05 09:05:38.106547400 +0400
+++ deepfloyd_if/pipelines/style_transfer.py	2023-06-04 16:12:41.387194300 +0400
@@ -13,3 +13,3 @@
     if_I,
-    if_II,
+    if_II=None,
     if_III=None,
@@ -36,7 +36,13 @@
     if prompt is not None:
-        t5_embs = t5.get_text_embeddings(prompt)
+        t5_embs = if_I_kwargs.get('t5_embs', None)
+        if t5_embs is None:
+            t5_embs = t5.get_text_embeddings(prompt)
     else:
-        t5_embs = t5.get_text_embeddings(style_prompt)
-
-    style_t5_embs = t5.get_text_embeddings(style_prompt)
+        t5_embs = if_I_kwargs.get('style_t5_embs', None)
+        if t5_embs is None:
+            t5_embs = t5.get_text_embeddings(style_prompt)
+
+    style_t5_embs = if_I_kwargs.get('style_t5_embs', None)
+    if t5_embs is None:
+        style_t5_embs = t5.get_text_embeddings(style_prompt)

@@ -45,3 +51,5 @@
             negative_prompt = [negative_prompt]
-        negative_t5_embs = t5.get_text_embeddings(negative_prompt)
+        negative_t5_embs = if_I_kwargs.get('negative_t5_embs', None)
+        if negative_t5_embs is None:
+            negative_t5_embs = t5.get_text_embeddings(negative_prompt)
     else:
@@ -49,6 +57,7 @@

-    low_res = _prepare_pil_image(support_pil_img, 64)
-    mid_res = _prepare_pil_image(support_pil_img, 256)
+    low_res = getattr(if_I_kwargs, 'low_res') if hasattr(if_I_kwargs, 'low_res') else _prepare_pil_image(support_pil_img, 64)
+    mid_res = getattr(if_I_kwargs, 'mid_res') if hasattr(if_I_kwargs, 'mid_res') else _prepare_pil_image(support_pil_img, 256)
     # high_res = _prepare_pil_image(support_pil_img, 1024)

+    stageI_output = None
     result = {}
@@ -68,4 +77,10 @@

-        stageI_generations, _ = if_I.embeddings_to_image(**if_I_kwargs)
-        pil_images_I = if_I.to_images(stageI_generations, disable_watermark=disable_watermark)
+        pass_prompt_to_sIII = True
+        if if_I_kwargs and hasattr(if_I_kwargs, "imagesI"):
+            stageI_generations = if_I_kwargs.tensorsI
+            pil_images_I = if_I_kwargs.imagesI
+            pass_prompt_to_sIII = if_I_kwargs.pass_prompt_to_sIII
+        else:
+            stageI_generations, _ = if_I.embeddings_to_image(**if_I_kwargs)
+            pil_images_I = if_I.to_images(stageI_generations, disable_watermark=disable_watermark)

@@ -90,3 +105,4 @@

-        stageII_generations, _meta = if_II.embeddings_to_image(**if_II_kwargs)
+        stageII_output = if_II.embeddings_to_image(**if_II_kwargs)
+        stageII_generations, _meta = stageII_output
         pil_images_II = if_II.to_images(stageII_generations, disable_watermark=disable_watermark)
@@ -95,4 +111,6 @@
     else:
+        stageII_output = None
         stageII_generations = None

+    stageIII_output = None
     if if_II is not None and if_III is not None:
@@ -103,3 +121,6 @@
             if if_III.use_diffusers:
-                if_III_kwargs['prompt'] = prompt[idx: idx+1] if prompt is not None else style_prompt[idx: idx+1]
+                if pass_prompt_to_sIII:
+                    if_III_kwargs['prompt'] = prompt[idx: idx+1] if prompt is not None else style_prompt[idx: idx+1]
+                else:
+                    if_III_kwargs['prompt'] = ['']

@@ -118,3 +139,4 @@

-            _stageIII_generations, _meta = if_III.embeddings_to_image(**if_III_kwargs)
+            stageIII_output = if_III.embeddings_to_image(**if_III_kwargs)
+            _stageIII_generations, _meta = stageIII_output
             stageIII_generations.append(_stageIII_generations)
@@ -129,2 +151,3 @@
     if return_tensors:
+        result["output"] = (stageI_output, stageII_output, stageIII_output)
         return result, (stageI_generations, stageII_generations, stageIII_generations)
diff -r '--unified=1' -x '*.pyc' deepfloyd_if-orig/pipelines/super_resolution.py deepfloyd_if/pipelines/super_resolution.py
--- deepfloyd_if-orig/pipelines/super_resolution.py	2023-06-05 09:05:38.106547400 +0400
+++ deepfloyd_if/pipelines/super_resolution.py	2023-06-04 16:12:41.368791600 +0400
@@ -30,3 +30,5 @@
     if prompt is not None:
-        t5_embs = t5.get_text_embeddings(prompt)
+        t5_embs = getattr(if_III_kwargs, 't5_embs', None)
+        if t5_embs is None:
+            t5_embs = t5.get_text_embeddings(prompt)
     else:
@@ -37,3 +39,5 @@
             negative_prompt = [negative_prompt]
-        negative_t5_embs = t5.get_text_embeddings(negative_prompt)
+        negative_t5_embs = getattr(if_III_kwargs, 'negative_t5_embs', None)
+        if negative_t5_embs is None:
+            negative_t5_embs = t5.get_text_embeddings(negative_prompt)
     else:
@@ -58,3 +62,4 @@

-    stageIII_generations, _meta = if_III.embeddings_to_image(**if_III_kwargs)
+    stageIII_output = if_III.embeddings_to_image(**if_III_kwargs)
+    stageIII_generations, _meta = stageIII_output
     pil_images_III = if_III.to_images(stageIII_generations, disable_watermark=disable_watermark)
@@ -63,2 +68,3 @@
     if return_tensors:
+        result["output"] = ([], [], stageIII_output)
         return result, (stageIII_generations,)
